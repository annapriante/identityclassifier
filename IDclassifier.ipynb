{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social identity classification using Machine Learning\n",
    "\n",
    "#### Loading required packages\n",
    "\n",
    "* Pandas\n",
    "* SKlearn\n",
    "* Numpy\n",
    "* NLTK\n",
    "* BeautifulSoup\n",
    "* RE (Regular Experssions)\n",
    "\n",
    "Anaconda Python distribution will widely used packages can be downloaded from here: https://www.continuum.io/downloads\n",
    "\n",
    "Available package list can be seen here: https://docs.continuum.io/anaconda/pkg-docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This method perform text pre processing like lower casing and stop word removal and returned clean sentence in array.\n",
    "def preProcessing(features):\n",
    "    num_titles = features.size\n",
    "    clean_titles = []\n",
    "    porter = nltk.PorterStemmer()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    for i in range( 0, num_titles):\n",
    "        text = BeautifulSoup(features[i], \"lxml\").get_text() \n",
    "        #letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "        words = text.lower().split()\n",
    "        clean_words = [w.lower() for w in words if not w in stops]  \n",
    "        #stemmed_words = [porter.stem(w) for w in clean_words]\n",
    "        clean_titles.append(\" \".join(clean_words))\n",
    "    return clean_titles\n",
    "\n",
    "# This method will perform feature extraction using TF-IDF and return the matrix and vectorizer that will be used to \n",
    "# transform new test examples before doing classification\n",
    "def getDTMByTFIDF(features,nfeatures):\n",
    "    tfIdf_vectorizer = TfidfVectorizer(max_features=nfeatures)\n",
    "    dtm = tfIdf_vectorizer.fit_transform(features).toarray()\n",
    "    return dtm,tfIdf_vectorizer\n",
    "\n",
    "# This method will take TF-IDF matrix along with class labels and output highly informative features selected using Chi^2 \n",
    "# along with Chi^2 model that will be used to transform new test examples before classification\n",
    "def featuresByChiSq(features,labels,nFeature=1000):\n",
    "    chi2_model = SelectKBest(chi2,k=nFeature)\n",
    "    dtm = chi2_model.fit_transform(features,labels)\n",
    "    return dtm,chi2_model\n",
    "\n",
    "# This method perform LSA/LSI and return LSA components (new learned features using LSI) along with LSA model\n",
    "def featuresByLSA(features,ncomponents=100):\n",
    "    svd = TruncatedSVD(n_components=ncomponents)\n",
    "    normalizer =  Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    dtm_lsa = lsa.fit_transform(features)\n",
    "    return dtm_lsa, lsa\n",
    "\n",
    "# This method takes document term matrix (usually output from Chi^2 or LSA), class labels and classifier object and number of folds. \n",
    "# It performs stratified CV and ouput average precision, recall and f-score.\n",
    "def crossValidate(document_term_matrix,labels,classifier,nfold=2):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "    skf = StratifiedKFold(labels, n_folds=nfold)  \n",
    "    for train_index, test_index in skf:\n",
    "        X_train, X_test = document_term_matrix[train_index], document_term_matrix[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        X_train_selected, chi2_model = featuresByChiSq(X_train,y_train,1000)       \n",
    "        X_test_selected = chi2_model.transform(X_test)\n",
    "        classifier.fit(X_train_selected, y_train)\n",
    "        y_pred = classifier.predict(X_test_selected)\n",
    "        p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        fscore.append(f)     \n",
    "    return (round(np.mean(precision),3),round(np.mean(recall),3),round(np.mean(fscore),3))\n",
    "\n",
    "# This method pretty print the confusion matrix. Extracted from some Github repo, forgot the name.\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    columnwidth = max([len(x) for x in labels]+[5]) # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print (\"    \" + empty_cell,)\n",
    "    for label in labels: \n",
    "        print (\"%{0}s\".format(columnwidth) % label,)\n",
    "    print\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print (\"    %{0}s\".format(columnwidth) % label1,)\n",
    "        for j in range(len(labels)): \n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print (cell,)\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading training set using Pandas (Make sure to save Excel file first as CSV, otherwise this will fail)\n",
    "training_data = pd.read_csv('Train5_dataset.csv', header=0, encoding='latin-1')\n",
    "features = training_data['from_user_description']\n",
    "\n",
    "relational_labels = training_data['Relational']\n",
    "occupation_labels = training_data['Occupation']\n",
    "political_labels = training_data['Political']\n",
    "ethnic_labels = training_data['Ethnic/religion']\n",
    "stigma_labels = training_data['Stigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading testing set using Pandas (Make sure to save Excel file first as CSV, otherwise this will fail)\n",
    "testing_data = pd.read_csv('Test5_dataset.csv', header=0, encoding='latin-1')\n",
    "testing_features = testing_data['from_user_description']\n",
    "\n",
    "testing_relational_labels = testing_data['Relational']\n",
    "testing_occupation_labels = testing_data['Occupation']\n",
    "testing_political_labels = testing_data['Political']\n",
    "testing_ethnic_labels = testing_data['Ethnic/religion']\n",
    "testing_stigma_labels = testing_data['Stigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text preprocessing of training and test set. Also creating TF-IDF matrix for training set\n",
    "processed_features = preProcessing(features)\n",
    "document_term_matrix,tfidf_vectorizer = getDTMByTFIDF(processed_features,None)\n",
    "test_processed_features = preProcessing(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is written to quickly perform different experiements, to select the best optimal model for each of the class labels i.e Relational & Political etc.\n",
    "\n",
    "##### Perform Chi^2 feature selection using different number of features and cross validate the results\n",
    "For the 10 fold cross validation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use differenet features selected via chi^2 feature selection and perform cross validation\n",
    "\n",
    "def training_id(name, lb, clf):\n",
    "    n_features = (1000,)\n",
    "    for n in n_features:\n",
    "        #selected_dtm, chi2_model = featuresByChiSq(document_term_matrix,lb,n)\n",
    "        evaluation_results = crossValidate(document_term_matrix,lb,clf,nfold=10)\n",
    "        print (name, n,)\n",
    "        print (\"{0}\\t{1}\\t{2}\".format(*evaluation_results))\n",
    "\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "bnb = BernoulliNB()\n",
    "rf = RandomForestClassifier(n_estimators = 100,class_weight=\"balanced_subsample\")\n",
    "svmc = svm.LinearSVC(C=0.5,class_weight=\"balanced\",max_iter=2000)\n",
    "\n",
    "training_id(\"relational SVM\", relational_labels, svmc)\n",
    "training_id(\"relational BNB\", relational_labels, bnb)\n",
    "training_id(\"relational LR\", relational_labels, lr)\n",
    "training_id(\"relational RF\", relational_labels, rf)\n",
    "training_id(\"occupation SVM\", occupation_labels, svmc)\n",
    "training_id(\"occupation BNB\", occupation_labels, bnb)\n",
    "training_id(\"occupation LR\", occupation_labels, lr)\n",
    "training_id(\"occupation RF\", occupation_labels, rf)\n",
    "training_id(\"political SVM \", political_labels, svmc)\n",
    "training_id(\"political BNB \", political_labels, bnb)\n",
    "training_id(\"political LR  \", political_labels, lr)\n",
    "training_id(\"political RF  \", political_labels, rf)\n",
    "training_id(\"ethnic SVM \", ethnic_labels, svmc)\n",
    "training_id(\"ethnic BNB \", ethnic_labels, bnb)\n",
    "training_id(\"ethnic LR  \", ethnic_labels, lr)\n",
    "training_id(\"ethnic RF  \", ethnic_labels, rf)\n",
    "training_id(\"stigma SVM \", stigma_labels, svmc)\n",
    "training_id(\"stigma BNB \", stigma_labels, bnb)\n",
    "training_id(\"stigma LR  \", stigma_labels, lr)\n",
    "training_id(\"stigma RF  \", stigma_labels, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combined features using tfidf, chi2 or information gain feature selection and latent sematic analysis\n",
    "Combined features do not significantly improve the Chi2 performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = stigma_labels\n",
    "svm_l_stigma = svm.LinearSVC(C=0.5,class_weight=\"balanced\",max_iter=2000)\n",
    "\n",
    "feature_union = FeatureUnion(\n",
    "    [('description', Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('best', SelectKBest(chi2,k=1200)),\n",
    "        #('best',SelectFromModel(tree.DecisionTreeClassifier(criterion=\"entropy\")))\n",
    "    ])),\n",
    "    ('lsa', Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('best', TruncatedSVD(n_components=10)),\n",
    "    ]))\n",
    "])\n",
    "combined_features = feature_union.fit_transform(processed_features,lb)\n",
    "\n",
    "print (\"-----SUPPORT VECTOR MACHINE-----\")\n",
    "svm_l = svm.LinearSVC(C=0.5,class_weight=\"balanced\",max_iter=1000)\n",
    "evaluation_results = crossValidate(combined_features,lb,svm_l_stigma,nfold=10)\n",
    "print (\"Precision: {0}\\nRecall: {1}\\nF-score: {2}\\n\".format(*evaluation_results))\n",
    "\n",
    "print (\"-----BERNOULLI NAIVE BAYES-----\")\n",
    "evaluation_results = crossValidate(combined_features,lb,bnb,nfold=10)\n",
    "print (\"Precision: {0}\\nRecall: {1}\\nF-score: {2}\\n\".format(*evaluation_results))\n",
    "\n",
    "print (\"-----LOGISTIC REGRESSION-----\")\n",
    "evaluation_results = crossValidate(combined_features,lb,lr,nfold=10)\n",
    "print (\"Precision: {0}\\nRecall: {1}\\nF-score: {2}\\n\".format(*evaluation_results))\n",
    "\n",
    "print (\"-----RANDOM FOREST-----\")\n",
    "evaluation_results = crossValidate(combined_features,lb,rf,nfold=10)\n",
    "print (\"Precision: {0}\\nRecall: {1}\\nF-score: {2}\\n\".format(*evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will perform feature selection using Chi^2 to learn 1000 highly informative features, transform the test set, \n",
    "train the model on training set and calculate precision, recall and F-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Classes Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_id(name, lb, clf, test_lb):\n",
    "    selected_dtm, chi2_model = featuresByChiSq(document_term_matrix, lb, 1000)\n",
    "    test_dtm = chi2_model.transform(tfidf_vectorizer.transform(test_processed_features))\n",
    "\n",
    "    clf.fit(selected_dtm, lb)\n",
    "    prediction = clf.predict(test_dtm)\n",
    "\n",
    "    p = precision_score(test_lb, prediction)\n",
    "    r = recall_score(test_lb, prediction)\n",
    "    f = 2*p*r / (p+r)\n",
    "    cm = confusion_matrix(test_lb, prediction, labels=[1,0])\n",
    "\n",
    "    print (name)\n",
    "    print (\"{0}\\t {1}\\t {2}\".format(p,r,f))\n",
    "    #print (\"Confusion Matrix\\n\")\n",
    "    #print_cm(cm, [[1],[0]])\n",
    "\n",
    "# ALSO ABOVE\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "bnb = BernoulliNB()\n",
    "rf = RandomForestClassifier(n_estimators = 100,class_weight=\"balanced_subsample\")\n",
    "svmc = svm.LinearSVC(C=0.5,class_weight=\"balanced\",max_iter=2000)\n",
    "\n",
    "test_id(\"Relational SVM\", relational_labels, svmc, testing_relational_labels)\n",
    "test_id(\"Relational BNB\",  relational_labels, bnb, testing_relational_labels)\n",
    "test_id(\"Relational LR\",  relational_labels, lr, testing_relational_labels)\n",
    "test_id(\"Relational RF\",  relational_labels, rf,  testing_relational_labels)\n",
    "test_id(\"Occupation SVM\",  occupation_labels, svmc, testing_occupation_labels)\n",
    "test_id(\"Occupation BNB\",  occupation_labels, bnb, testing_occupation_labels)\n",
    "test_id(\"Occupation LR\",  occupation_labels, lr, testing_occupation_labels)\n",
    "test_id(\"Occupation RF\",  occupation_labels, rf, testing_occupation_labels)\n",
    "test_id(\"Political SVM\",  political_labels, svmc, testing_political_labels)\n",
    "test_id(\"Political BNB\",  political_labels, bnb, testing_political_labels)\n",
    "test_id(\"Political LR\",  political_labels, lr, testing_political_labels)\n",
    "test_id(\"Political RF \",  political_labels,  rf, testing_political_labels)\n",
    "test_id(\"Ethnic SVM\",  ethnic_labels, svmc, testing_ethnic_labels)\n",
    "test_id(\"Ethnic BNB\",  ethnic_labels, bnb, testing_ethnic_labels)\n",
    "test_id(\"Ethnic LR\",  ethnic_labels, lr, testing_ethnic_labels)\n",
    "test_id(\"Ethnic RF \",  ethnic_labels,  rf, testing_ethnic_labels)\n",
    "test_id(\"Stigma SVM\",  stigma_labels, svmc, testing_stigma_labels)\n",
    "test_id(\"Stigma BNB\",  stigma_labels, bnb, testing_stigma_labels)\n",
    "test_id(\"Stigma LR\",  stigma_labels, lr, testing_stigma_labels)\n",
    "test_id(\"Stigma RF \",  stigma_labels,  rf, testing_stigma_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
